{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa472280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Cell 1: Setup and Imports\n",
    "# ===================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For iSOM Data Generation\n",
    "from minisom import MiniSom\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For TabPFN Model\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# --- Installation Note ---\n",
    "# !pip install tabpfn minisom sklearn pandas seaborn matplotlib tqdm\n",
    "\n",
    "print(\"Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7bbdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: Helper Functions (iSOM Generator and Data Loaders)\n",
    "# ===================================================================\n",
    "\n",
    "def generate_isom_data(df_real, target_column, num_synthetic):\n",
    "    if num_synthetic <= 0:\n",
    "        return pd.DataFrame(columns=df_real.columns)\n",
    "    \n",
    "    X_real = df_real.drop(columns=[target_column]).values\n",
    "    y_real = df_real[target_column].values\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X_real)\n",
    "\n",
    "    map_size = int(np.sqrt(5 * np.sqrt(X_scaled.shape[0])))\n",
    "    som_grid_size = (map_size, map_size)\n",
    "    \n",
    "    som = MiniSom(som_grid_size[0], som_grid_size[1], X_scaled.shape[1],\n",
    "                  sigma=1.0, learning_rate=0.5, random_seed=42)\n",
    "    som.random_weights_init(X_scaled)\n",
    "    som.train_random(X_scaled, 500)\n",
    "\n",
    "    def get_neighbors(x, y, grid_size):\n",
    "        neighbors = []\n",
    "        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < grid_size[0] and 0 <= ny < grid_size[1]:\n",
    "                neighbors.append((nx, ny))\n",
    "        return neighbors\n",
    "\n",
    "    weights = som.get_weights()\n",
    "    synthetic_samples = []\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    for _ in range(num_synthetic):\n",
    "        x1, y1 = rng.integers(som_grid_size[0]), rng.integers(som_grid_size[1])\n",
    "        valid_neighbors = get_neighbors(x1, y1, som_grid_size)\n",
    "        if not valid_neighbors: continue\n",
    "        x2, y2 = rng.choice(valid_neighbors)\n",
    "        node1_weights, node2_weights = weights[x1, y1], weights[x2, y2]\n",
    "        alpha = rng.random()\n",
    "        new_sample_scaled = alpha * node1_weights + (1 - alpha) * node2_weights\n",
    "        distances = np.linalg.norm(X_scaled - new_sample_scaled, axis=1)\n",
    "        new_target = y_real[np.argmin(distances)]\n",
    "        synthetic_samples.append(np.append(new_sample_scaled, new_target))\n",
    "\n",
    "    synthetic_df = pd.DataFrame(synthetic_samples, columns=df_real.columns)\n",
    "    synthetic_features_scaled = synthetic_df.drop(columns=[target_column]).values\n",
    "    synthetic_features_orig = scaler.inverse_transform(synthetic_features_scaled)\n",
    "    synthetic_df[df_real.drop(columns=[target_column]).columns] = synthetic_features_orig\n",
    "    synthetic_df[target_column] = synthetic_df[target_column].astype(int)\n",
    "    return synthetic_df\n",
    "\n",
    "def read_data(path):\n",
    "    _, ext = os.path.splitext(path)\n",
    "    if ext == \".parquet\": return pd.read_parquet(path)\n",
    "    elif ext == \".csv\": return pd.read_csv(path, index_col=0)\n",
    "\n",
    "def load_data(base_path):\n",
    "    _, dataset_name = os.path.split(base_path)\n",
    "    with open(os.path.join(base_path, f\"{dataset_name}.meta.json\")) as f: meta = json.load(f)\n",
    "    train_data = read_data(os.path.join(base_path, f\"{dataset_name}.{meta['format']}\"))\n",
    "    test_path = os.path.join(base_path, f\"{dataset_name}_test.{meta['format']}\")\n",
    "    test_data = read_data(test_path) if os.path.exists(test_path) else train_data\n",
    "    return train_data, test_data, meta\n",
    "\n",
    "def get_indices_for_repeat(base_path, repeat_idx):\n",
    "    train_idx_file = os.path.join(base_path, \"train_indices.parquet\")\n",
    "    train_idx_splits = pd.read_parquet(train_idx_file)\n",
    "    col_name = train_idx_splits.columns[repeat_idx]\n",
    "    return train_idx_splits[col_name].values\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d55d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: Main Configuration\n",
    "# ===================================================================\n",
    "BASE_DATA_PATH = \"data\"\n",
    "DATASETS_TO_RUN = [\n",
    "    \"truss_6d\"\n",
    "]\n",
    "\n",
    "REPEATS =1 # Number of random shuffles to run for each experiment\n",
    "\n",
    "# --- Experiment Parameters ---\n",
    "# We will run the augmentation experiment for all these starting fractions\n",
    "FIXED_REAL_FRACTIONS = [0.1, 0.3] \n",
    "# And add these amounts of synthetic data\n",
    "SYNTHETIC_ADD_FRACTIONS = np.arange(0.0, 0.4, 0.1).round(1)\n",
    "# The baseline will cover all relevant real data fractions\n",
    "BASELINE_REAL_FRACTIONS = np.arange(0.1, 0.6, 0.1).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2aa9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd10f1d9d2b4208969a4f5c65dc8acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Dataset Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running Dataset: truss_6d =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johnn\\Documents\\From Desktop\\All things Python\\env\\lib\\site-packages\\tabpfn\\architectures\\base\\attention\\full_attention.py:678: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attention_head_outputs = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 0 synthetic samples for truss_6d (repeat 1, start 0.1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnn\\AppData\\Local\\Temp\\ipykernel_24724\\457590539.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_augmented = pd.concat([train_subset_real, train_subset_synthetic], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 synthetic samples for truss_6d (repeat 1, start 0.1)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of samples 11000 in the input data is greater than the maximum number of samples 10000 officially supported by TabPFN. Set `ignore_pretraining_limits=True` to override this error!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m X_train \u001b[38;5;241m=\u001b[39m train_augmented[X_columns]\n\u001b[0;32m     61\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train_augmented[y_column]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     66\u001b[0m score \u001b[38;5;241m=\u001b[39m f1_score(y_test\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\johnn\\Documents\\From Desktop\\All things Python\\env\\lib\\site-packages\\tabpfn\\classifier.py:631\u001b[0m, in \u001b[0;36mTabPFNClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdifferentiable_input:\n\u001b[0;32m    630\u001b[0m     byte_size, rng \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_model_variables()\n\u001b[1;32m--> 631\u001b[0m     ensemble_configs, X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_dataset_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# already fitted and prompt_tuning mode: no cat. features\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     _, rng \u001b[38;5;241m=\u001b[39m infer_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32mc:\\Users\\johnn\\Documents\\From Desktop\\All things Python\\env\\lib\\site-packages\\tabpfn\\classifier.py:455\u001b[0m, in \u001b[0;36mTabPFNClassifier._initialize_dataset_preprocessing\u001b[1;34m(self, X, y, rng)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_dataset_preprocessing\u001b[39m(\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    446\u001b[0m     X: XType,\n\u001b[0;32m    447\u001b[0m     y: YType,\n\u001b[0;32m    448\u001b[0m     rng,  \u001b[38;5;66;03m# noqa: ANN001\u001b[39;00m\n\u001b[0;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[ClassifierEnsembleConfig], XType, YType]:\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal preprocessing method for input arguments.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    Returns ClassifierEnsembleConfigs, inferred categorical indices,\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    and modelfied features X and labels y.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    Sets self.inferred_categorical_indices_.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     X, y, feature_names_in, n_features_in \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_Xy_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_y_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_num_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface_config_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_NUMBER_OF_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_num_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface_config_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_NUMBER_OF_FEATURES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_pretraining_limits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_pretraining_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m     check_cpu_warning(\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, X, allow_cpu_override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_pretraining_limits\n\u001b[0;32m    467\u001b[0m     )\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_names_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\johnn\\Documents\\From Desktop\\All things Python\\env\\lib\\site-packages\\tabpfn\\utils.py:452\u001b[0m, in \u001b[0;36mvalidate_Xy_fit\u001b[1;34m(X, y, estimator, max_num_features, max_num_samples, ensure_y_numeric, ignore_pretraining_limits)\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the input data is greater than \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe maximum number of features \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_num_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m officially \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by the TabPFN model. Set `ignore_pretraining_limits=True` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto override this error!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m max_num_samples \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_pretraining_limits:\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of samples \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the input data is greater than \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe maximum number of samples \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m officially supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m by TabPFN. Set `ignore_pretraining_limits=True` to override this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_classifier(estimator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39mdifferentiable_input:\n\u001b[0;32m    460\u001b[0m     check_classification_targets(y)\n",
      "\u001b[1;31mValueError\u001b[0m: Number of samples 11000 in the input data is greater than the maximum number of samples 10000 officially supported by TabPFN. Set `ignore_pretraining_limits=True` to override this error!"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: Main Experiment Loop\n",
    "# ===================================================================\n",
    "all_results = []\n",
    "classifier = TabPFNClassifier(device=\"cuda\")\n",
    "\n",
    "for dataset_name in tqdm(DATASETS_TO_RUN, desc=\"Overall Dataset Progress\"):\n",
    "    print(f\"\\n===== Running Dataset: {dataset_name} =====\")\n",
    "    dataset_path = os.path.join(BASE_DATA_PATH, dataset_name)\n",
    "    \n",
    "    # Load data for the current dataset\n",
    "    train_data, test_data, meta = load_data(dataset_path)\n",
    "    X_columns, y_column = train_data.columns.drop(meta[\"label\"]), meta[\"label\"]\n",
    "    X_test, y_test = test_data[X_columns], test_data[y_column]\n",
    "    full_train_size = len(train_data)\n",
    "    \n",
    "    # This dataset may have a continuous target, so we convert it to binary classes\n",
    "    if pd.api.types.is_float_dtype(train_data[y_column]):\n",
    "        print(f\"Dataset '{dataset_name}' has a continuous target. Converting to classes...\")\n",
    "        threshold = train_data[y_column].median()\n",
    "        train_data[y_column] = (train_data[y_column] <= threshold).astype(int)\n",
    "        test_data[y_column] = (test_data[y_column] <= threshold).astype(int)\n",
    "        y_test = test_data[y_column] # Re-assign y_test with the new classes\n",
    "\n",
    "    for i in range(REPEATS):\n",
    "        shuffled_indices = get_indices_for_repeat(dataset_path, i)\n",
    "        \n",
    "        # --- Baseline Experiment ---\n",
    "        for real_fraction in BASELINE_REAL_FRACTIONS:\n",
    "            n_real = int(real_fraction * full_train_size)\n",
    "            train_indices = shuffled_indices[:n_real]\n",
    "            \n",
    "            X_train = train_data.loc[train_indices, X_columns]\n",
    "            y_train = train_data.loc[train_indices, y_column].astype(int)\n",
    "            \n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            \n",
    "            score = f1_score(y_test.astype(int), y_pred, average=\"macro\")\n",
    "            all_results.append({\"dataset\": dataset_name,\n",
    "                                \"experiment\": \"Baseline (Real Only)\", \n",
    "                                \"total_fraction\": real_fraction, \n",
    "                                \"f1_score\": score})\n",
    "\n",
    "        # --- Incremental Augmentation Experiment ---\n",
    "        for real_fraction in FIXED_REAL_FRACTIONS:\n",
    "            n_real = int(real_fraction * full_train_size)\n",
    "            real_indices = shuffled_indices[:n_real]\n",
    "            train_subset_real = train_data.loc[real_indices]\n",
    "            \n",
    "            for synthetic_fraction in SYNTHETIC_ADD_FRACTIONS:\n",
    "                n_synthetic = int(synthetic_fraction * full_train_size)\n",
    "                \n",
    "                # Generate synthetic data based on the fixed real subset\n",
    "                print(f\"Generating {n_synthetic} synthetic samples for {dataset_name} (repeat {i+1}, start {real_fraction})...\")\n",
    "                train_subset_synthetic = generate_isom_data(train_subset_real, y_column, n_synthetic)\n",
    "                \n",
    "                # Combine for training\n",
    "                train_augmented = pd.concat([train_subset_real, train_subset_synthetic], ignore_index=True)\n",
    "                X_train = train_augmented[X_columns]\n",
    "                y_train = train_augmented[y_column].astype(int)\n",
    "                \n",
    "                classifier.fit(X_train, y_train)\n",
    "                y_pred = classifier.predict(X_test)\n",
    "                \n",
    "                score = f1_score(y_test.astype(int), y_pred, average=\"macro\")\n",
    "                total_fraction = round(real_fraction + synthetic_fraction, 1)\n",
    "                all_results.append({\"dataset\": dataset_name,\n",
    "                                    \"experiment\": f\"Start {int(real_fraction*100)}% Real + iSOM\", \n",
    "                                    \"total_fraction\": total_fraction, \n",
    "                                    \"f1_score\": score})\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\n\\n===== ALL EXPERIMENTS COMPLETE! =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde265c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: Plotting - Set 1 (Start Fractions 10% & 20%) (Corrected)\n",
    "# ===================================================================\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "experiments_to_plot_1 = [\n",
    "    \"Baseline (Real Only)\",\n",
    "    \"Start 10% Real + iSOM\"\n",
    "]\n",
    "df_plot_1 = results_df[results_df['experiment'].isin(experiments_to_plot_1)]\n",
    "\n",
    "# Create the FacetGrid (this is correct)\n",
    "g1 = sns.FacetGrid(\n",
    "    df_plot_1, \n",
    "    col=\"dataset\", \n",
    "    col_wrap=1, \n",
    "    hue=\"experiment\",\n",
    "    hue_order=experiments_to_plot_1,\n",
    "    col_order=DATASETS_TO_RUN,\n",
    "    height=4, \n",
    "    aspect=1.2, \n",
    "    sharey=False\n",
    ")\n",
    "\n",
    "# --- CORRECTED FIX ---\n",
    "# Remove the 'style' argument from the map_dataframe call.\n",
    "# 'hue' is already handling the separation of lines.\n",
    "g1.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"total_fraction\",\n",
    "    y=\"f1_score\",\n",
    "    markers=True, # markers are still good for clarity\n",
    "    errorbar=\"sd\"\n",
    ").set_titles(\"{col_name}\")\n",
    "\n",
    "g1.set_axis_labels(\"Total Training Fraction (Real + Synthetic)\", \"F1 Score (Macro)\")\n",
    "g1.fig.suptitle(\"Incremental Augmentation (Starting Fractions: 10% & 20%)\", y=1.03, fontsize=16)\n",
    "g1.add_legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f93651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: Plotting - Set 2 (Start Fractions 30% & 40%) (Corrected)\n",
    "# ===================================================================\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "experiments_to_plot_2 = [\n",
    "    \"Baseline (Real Only)\",\n",
    "    \"Start 30% Real + iSOM\"\n",
    "]\n",
    "df_plot_2 = results_df[results_df['experiment'].isin(experiments_to_plot_2)]\n",
    "\n",
    "# Create the FacetGrid\n",
    "g2 = sns.FacetGrid(\n",
    "    df_plot_2, \n",
    "    col=\"dataset\", \n",
    "    col_wrap=1, \n",
    "    hue=\"experiment\",\n",
    "    hue_order=experiments_to_plot_2,\n",
    "    col_order=DATASETS_TO_RUN,\n",
    "    height=4, \n",
    "    aspect=1.2, \n",
    "    sharey=False\n",
    ")\n",
    "\n",
    "# --- CORRECTED FIX ---\n",
    "# Remove the 'style' argument here as well.\n",
    "g2.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"total_fraction\",\n",
    "    y=\"f1_score\",\n",
    "    markers=True,\n",
    "    errorbar=\"sd\"\n",
    ").set_titles(\"{col_name}\")\n",
    "\n",
    "g2.set_axis_labels(\"Total Training Fraction (Real + Synthetic)\", \"F1 Score (Macro)\")\n",
    "g2.fig.suptitle(\"Incremental Augmentation (Starting Fractions: 30% & 40%)\", y=1.03, fontsize=16)\n",
    "g2.add_legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
