{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 1: Setup and Imports\n",
    "# ===================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For TabPFN Model\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- SOMpy Imports ---\n",
    "# Make sure the sompy library files are in a 'folder' subdirectory\n",
    "# or adjust the import path accordingly.\n",
    "from folder.sompy_isom import SOMFactory\n",
    "from folder.aux_fun import aux_fun\n",
    "\n",
    "# --- Visualization Imports ---\n",
    "from folder.visualization.viz_functions import Visualization_func\n",
    "from bokeh.plotting import show\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Enable Bokeh plotting in the notebook\n",
    "output_notebook()\n",
    "\n",
    "# --- Installation Note ---\n",
    "# !pip install tabpfn sklearn pandas seaborn matplotlib tqdm sompy bokeh\n",
    "\n",
    "print(\"Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10eafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: NEW - Intelligent Synthetic Data Generator using sompy\n",
    "# ===================================================================\n",
    "# This cell replaces the old ModifiedSOM class and generator function.\n",
    "\n",
    "# --- Add visualize_hitmap parameter ---\n",
    "def generate_isom_data_advanced(df_real, target_column, num_synthetic_total, strategy=\"decision_boundary\", roi_percentile=75, boundary_width=0.25, visualize_hitmap=False, dataset_name_for_viz=\"\"):\n",
    "    \"\"\"\n",
    "    Generates synthetic data by starting from real data points and exploring towards \n",
    "    neighboring prototypes within the Region of Interest (RoI). Optionally visualizes the hitmap.\n",
    "\n",
    "    Args:\n",
    "        ... (other args remain the same) ...\n",
    "        visualize_hitmap (bool): If True and strategy is 'hitmap', shows the hitmap plot.\n",
    "        dataset_name_for_viz (str): Name of the dataset for the plot title.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the generated synthetic data.\n",
    "    \"\"\"\n",
    "    if num_synthetic_total <= 0:\n",
    "        return pd.DataFrame(columns=df_real.columns)\n",
    "\n",
    "    print(f\"Generating {num_synthetic_total} ADVANCED synthetic samples (Strategy: {strategy})...\")\n",
    "\n",
    "    # --- 1. Prepare Data and Train iSOM (Same as before) ---\n",
    "    X_real = df_real.drop(columns=[target_column])\n",
    "    y_real = df_real[target_column]\n",
    "    data_for_som = np.hstack((X_real.values, y_real.values.reshape(-1, 1)))\n",
    "    component_names = list(df_real.columns)\n",
    "\n",
    "    sm = SOMFactory.build(data_for_som, normalization='range', initialization='pca', component_names=component_names)\n",
    "    sm.som_lininit()\n",
    "    print(f\"  > Training SOM for {dataset_name_for_viz} ({strategy})...\")\n",
    "    sm.train(request_id=f'isom_gen_advanced_{strategy}', verbose=None)\n",
    "    \n",
    "    af = aux_fun()\n",
    "    codebook = sm.codebook.matrix\n",
    "\n",
    "    # --- 2. Identify Region of Interest (RoI) ---\n",
    "    hits = None # Initialize hits variable\n",
    "\n",
    "    if strategy == \"decision_boundary\":\n",
    "        print(f\"  > Defining RoI as decision boundary...\")\n",
    "        target_values = codebook[:, -1]\n",
    "        lower_bound, upper_bound = 0.5 - boundary_width, 0.5 + boundary_width\n",
    "        roi_node_indices = np.where((target_values >= lower_bound) & (target_values <= upper_bound))[0]\n",
    "    \n",
    "    elif strategy == \"hitmap\":\n",
    "        print(f\"  > Defining RoI from the top {100 - roi_percentile}% most hit nodes...\")\n",
    "        hits = af.som_hits(sm, sm._data) # Calculate hits\n",
    "        hit_threshold = np.percentile(hits[hits > 0], roi_percentile) if len(hits[hits > 0]) > 0 else 0\n",
    "        roi_node_indices = np.where(hits >= hit_threshold)[0]\n",
    "\n",
    "# --- <<< START: Visualization Block >>> ---\n",
    "        if visualize_hitmap and hits is not None:\n",
    "            try:\n",
    "                print(f\"  > Visualizing hitmap for {dataset_name_for_viz}...\")\n",
    "                vis = Visualization_func(sm)\n",
    "                # get_som_hitmap requires hits, comp='all' or indices, color='red' etc.\n",
    "                hit_obj = vis.get_som_hitmap(hits, comp='all', clr='red')\n",
    "                # plot_hitmap requires the hit_obj list and comp selection\n",
    "                hitmap_plot = vis.plot_hitmap(hit_obj, comp='all')\n",
    "\n",
    "                # --- <<< FIX: Remove this line >>> ---\n",
    "                # hitmap_plot.title = f\"Hitmap for {dataset_name_for_viz} (Strategy: {strategy})\"\n",
    "\n",
    "                show(hitmap_plot) # Display the plot\n",
    "            except Exception as e:\n",
    "                print(f\"  > Warning: Could not generate hitmap visualization. Error: {e}\")\n",
    "        # --- <<< END: Visualization Block >>> ---\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy.\")\n",
    "\n",
    "    if len(roi_node_indices) == 0:\n",
    "        print(\"Warning: No RoI nodes found. Using all prototypes as fallback.\")\n",
    "        roi_node_indices = np.arange(sm.codebook.nnodes)\n",
    "    \n",
    "    print(f\"  > RoI contains {len(roi_node_indices)} prototypes.\")\n",
    "\n",
    "    # --- 3. REFINED Synthetic Data Generation (Same as before) ---\n",
    "    synthetic_samples = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    neighborhood_matrix = af.som_unit_neighs(sm)\n",
    "    bmus = af.som_bmus(sm, data_for_som) \n",
    "\n",
    "    for _ in tqdm(range(num_synthetic_total), desc=\"  > Generating samples\", leave=False):\n",
    "        p1_idx = rng.choice(roi_node_indices)\n",
    "        p1 = codebook[p1_idx]\n",
    "        real_points_mapped_to_p1_indices = np.where(bmus == p1_idx)[0]\n",
    "        if len(real_points_mapped_to_p1_indices) == 0: continue\n",
    "        d_real_idx = rng.choice(real_points_mapped_to_p1_indices)\n",
    "        d_real_norm = sm._data[d_real_idx] \n",
    "        neighbor_indices = np.where(neighborhood_matrix[p1_idx] == 1)[0]\n",
    "        roi_neighbors = np.intersect1d(neighbor_indices, roi_node_indices)\n",
    "        p2 = codebook[rng.choice(roi_neighbors)] if len(roi_neighbors) > 0 else codebook[rng.choice(roi_node_indices)]\n",
    "        alpha = rng.random() \n",
    "        new_sample_norm = d_real_norm + alpha * (p2 - p1)\n",
    "        features_norm = new_sample_norm[:-1]\n",
    "        noise = rng.normal(0, 0.05, size=features_norm.shape)\n",
    "        features_norm_noisy = np.clip(features_norm + noise, 0, 1)\n",
    "        denormalized_data_wrapper = np.array([np.append(features_norm_noisy, 0)])\n",
    "        final_features = sm._normalizer.denormalize_by(data_for_som, denormalized_data_wrapper)[:, :-1]\n",
    "        target_prob_norm = np.clip(new_sample_norm[-1], 0, 1)\n",
    "        final_class = rng.binomial(1, target_prob_norm)\n",
    "        synthetic_samples.append(np.append(final_features.flatten(), final_class))\n",
    "\n",
    "    final_synthetic_df = pd.DataFrame(synthetic_samples, columns=df_real.columns)\n",
    "    final_synthetic_df[target_column] = final_synthetic_df[target_column].astype(int)\n",
    "\n",
    "    return final_synthetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: Helper Functions (Data Loading) - UNCHANGED\n",
    "# ===================================================================\n",
    "def read_data(path):\n",
    "    _, ext = os.path.splitext(path)\n",
    "    if ext == \".parquet\": return pd.read_parquet(path)\n",
    "    elif ext == \".csv\": return pd.read_csv(path, index_col=0)\n",
    "\n",
    "def load_data(base_path, dataset_name):\n",
    "    with open(os.path.join(base_path, dataset_name, f\"{dataset_name}.meta.json\")) as f: meta = json.load(f)\n",
    "    train_data = read_data(os.path.join(base_path, dataset_name, f\"{dataset_name}.{meta['format']}\"))\n",
    "    \n",
    "    test_path = os.path.join(base_path, dataset_name, f\"{dataset_name}_test.{meta['format']}\")\n",
    "    if os.path.exists(test_path):\n",
    "        print(f\"Found dedicated test set for {dataset_name}.\")\n",
    "        test_data = read_data(test_path)\n",
    "    else:\n",
    "        print(f\"No dedicated test set for {dataset_name}. Splitting will be handled by index file.\")\n",
    "        test_data = None\n",
    "        \n",
    "    return train_data, test_data, meta\n",
    "\n",
    "def get_indices_for_repeat(base_path, dataset_name, repeat_idx):\n",
    "    train_idx_file = os.path.join(base_path, dataset_name, \"train_indices.parquet\")\n",
    "    train_idx_splits = pd.read_parquet(train_idx_file)\n",
    "    col_name = train_idx_splits.columns[repeat_idx]\n",
    "    return train_idx_splits[col_name].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: Main Configuration - UNCHANGED\n",
    "# ===================================================================\n",
    "BASE_DATA_PATH = \"data\"\n",
    "DATASETS_TO_RUN = [\n",
    "    \"airfoil_cl\", \"airfoil_cl_m\", \"framed_safety\", \"framed_validity\", \n",
    "    \"solar_hex\", \"welded_beam\", \"welded_beam_balanced\"\n",
    "]\n",
    "REPEATS = 1\n",
    "\n",
    "# --- Experiment Parameters ---\n",
    "STARTING_REAL_FRACTIONS = [0.1, 0.3, 0.5]\n",
    "POOL_TRAINING_FRACTIONS = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "BASELINE_REAL_FRACTIONS = np.arange(0.1, 1.1, 0.1).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: Main Experiment Loop - MODIFIED for hitmap visualization\n",
    "# ===================================================================\n",
    "all_results = []\n",
    "classifier = TabPFNClassifier(device=\"cuda\")\n",
    "\n",
    "for dataset_name in tqdm(DATASETS_TO_RUN, desc=\"Overall Dataset Progress\"):\n",
    "    print(f\"\\n===== Running Dataset: {dataset_name} =====\")\n",
    "    \n",
    "    full_train_data, test_data_from_file, meta = load_data(BASE_DATA_PATH, dataset_name)\n",
    "    y_column = meta[\"label\"]\n",
    "\n",
    "    if pd.api.types.is_float_dtype(full_train_data[y_column]):\n",
    "        threshold = full_train_data[y_column].median()\n",
    "        full_train_data[y_column] = (full_train_data[y_column] <= threshold).astype(int)\n",
    "        if test_data_from_file is not None:\n",
    "            test_data_from_file[y_column] = (test_data_from_file[y_column] <= threshold).astype(int)\n",
    "\n",
    "    X_columns = full_train_data.columns.drop(y_column)\n",
    "    full_train_size = len(full_train_data)\n",
    "\n",
    "    for i in range(REPEATS):\n",
    "        shuffled_indices = get_indices_for_repeat(BASE_DATA_PATH, dataset_name, i)\n",
    "        \n",
    "        if test_data_from_file is not None:\n",
    "            X_test, y_test = test_data_from_file[X_columns], test_data_from_file[y_column]\n",
    "        else:\n",
    "            train_idx_for_split, test_idx_for_split = np.split(shuffled_indices, [int(0.8 * len(shuffled_indices))])\n",
    "            X_test, y_test = full_train_data.loc[test_idx_for_split, X_columns], full_train_data.loc[test_idx_for_split, y_column]\n",
    "\n",
    "        # --- Baseline Experiment (Unchanged) ---\n",
    "        for real_fraction in tqdm(BASELINE_REAL_FRACTIONS, desc=f\"Repeat {i+1} Baseline\", leave=False):\n",
    "            n_real = int(real_fraction * full_train_size)\n",
    "            if n_real < 2: continue\n",
    "            train_indices = shuffled_indices[:n_real]\n",
    "            X_train, y_train = full_train_data.loc[train_indices, X_columns], full_train_data.loc[train_indices, y_column].astype(int)\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            score = f1_score(y_test.astype(int), y_pred, average=\"macro\")\n",
    "            all_results.append({\"dataset\": dataset_name, \"experiment\": \"Baseline (Real Only)\", \"total_fraction\": real_fraction, \"f1_score\": score})\n",
    "\n",
    "        # --- Augmentation Experiment ---\n",
    "        AUGMENTATION_STRATEGIES = [\"decision_boundary\", \"hitmap\"] \n",
    "        for strategy in tqdm(AUGMENTATION_STRATEGIES, desc=f\"Repeat {i+1} Augmentation Strategies\", leave=False):\n",
    "            # --- <<< Set visualize flag based on strategy >>> ---\n",
    "            should_visualize = (strategy == \"hitmap\") \n",
    "\n",
    "            for start_fraction in tqdm(STARTING_REAL_FRACTIONS, desc=f\"  Start Fractions ({strategy})\", leave=False):\n",
    "                n_real = int(start_fraction * full_train_size)\n",
    "                if n_real < 2: continue\n",
    "                real_subset = full_train_data.loc[shuffled_indices[:n_real]]\n",
    "                \n",
    "                n_synthetic = full_train_size - n_real\n",
    "                # --- <<< Pass visualize flag and dataset name to generator >>> ---\n",
    "                synthetic_data = generate_isom_data_advanced(\n",
    "                    real_subset, \n",
    "                    y_column, \n",
    "                    n_synthetic, \n",
    "                    strategy=strategy, \n",
    "                    visualize_hitmap=should_visualize, \n",
    "                    dataset_name_for_viz=f\"{dataset_name}_Start{int(start_fraction*100)}%\" \n",
    "                )\n",
    "                data_pool = pd.concat([real_subset, synthetic_data]).sample(frac=1, random_state=i)\n",
    "                \n",
    "                for pool_fraction in POOL_TRAINING_FRACTIONS:\n",
    "                    n_pool_samples = int(pool_fraction * len(data_pool))\n",
    "                    if n_pool_samples < 2: continue\n",
    "                    training_subset_from_pool = data_pool.head(n_pool_samples)\n",
    "                    X_train, y_train = training_subset_from_pool[X_columns], training_subset_from_pool[y_column].astype(int)\n",
    "                    classifier.fit(X_train, y_train)\n",
    "                    y_pred = classifier.predict(X_test)\n",
    "                    score = f1_score(y_test.astype(int), y_pred, average=\"macro\")\n",
    "                    exp_name = f\"Augmented ({strategy})\"\n",
    "                    all_results.append({\"dataset\": dataset_name, \"experiment\": exp_name, \"start_fraction\": start_fraction, \"total_fraction\": pool_fraction, \"f1_score\": score})\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\n\\n===== ALL EXPERIMENTS COMPLETE! =====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: Visualize All Results - FINAL CORRECTED VERSION\n",
    "# ===================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# --- 1. Prepare the data for plotting ---\n",
    "\n",
    "# Calculate the starting F1 score for scatter plot points\n",
    "aug_df = results_df[results_df['experiment'] != \"Baseline (Real Only)\"].copy()\n",
    "aug_df['start_f1'] = aug_df.groupby(['dataset', 'experiment', 'start_fraction'])['f1_score'].transform('first')\n",
    "\n",
    "# If the start_f1 column already exists from a previous run, remove it\n",
    "if 'start_f1' in results_df.columns:\n",
    "    results_df = results_df.drop(columns=['start_f1'])\n",
    "\n",
    "# Merge the start_f1 column back into the main DataFrame\n",
    "results_df = results_df.merge(aug_df[['start_f1']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# --- FIX: Create a new, more descriptive column for the legend ---\n",
    "def create_exp_label(row):\n",
    "    if \"Augmented\" in row['experiment']:\n",
    "        start_pct = int(row['start_fraction'] * 100)\n",
    "        strategy = row['experiment'].split('(')[1].split(')')[0]\n",
    "        return f\"Augmented ({strategy}, Start {start_pct}%)\"\n",
    "    return \"Baseline (Real Only)\"\n",
    "\n",
    "results_df['exp_label'] = results_df.apply(create_exp_label, axis=1)\n",
    "\n",
    "\n",
    "# --- 2. Loop through each strategy to create separate plots ---\n",
    "AUGMENTATION_STRATEGIES = [\"hitmap\"]\n",
    "\n",
    "for strategy in AUGMENTATION_STRATEGIES:\n",
    "    \n",
    "    # Define the order for the legend in this specific plot\n",
    "    experiment_order = (\n",
    "        [\"Baseline (Real Only)\"] + \n",
    "        [f\"Augmented ({strategy}, Start {int(f*100)}%)\" for f in STARTING_REAL_FRACTIONS]\n",
    "    )\n",
    "    \n",
    "    # Filter the data for the current strategy and the baseline\n",
    "    plot_df = results_df[\n",
    "        results_df['experiment'].isin([\"Baseline (Real Only)\", f\"Augmented ({strategy})\"])\n",
    "    ]\n",
    "\n",
    "    # Create the FacetGrid\n",
    "    g = sns.FacetGrid(\n",
    "        plot_df, \n",
    "        col=\"dataset\", \n",
    "        col_wrap=4, \n",
    "        hue=\"exp_label\",  # <-- Use the new descriptive label for color\n",
    "        hue_order=experiment_order,\n",
    "        col_order=DATASETS_TO_RUN,\n",
    "        height=4, \n",
    "        aspect=1.2, \n",
    "        sharey=False,\n",
    "        legend_out=True\n",
    "    )\n",
    "\n",
    "    # Map the lineplot\n",
    "    g.map_dataframe(\n",
    "        sns.lineplot,\n",
    "        x=\"total_fraction\",\n",
    "        y=\"f1_score\",\n",
    "        markers=True,\n",
    "        errorbar=\"sd\"\n",
    "    ).set_titles(\"{col_name}\")\n",
    "\n",
    "    # Map the scatterplot for starting points\n",
    "    g.map_dataframe(\n",
    "        sns.scatterplot,\n",
    "        x='start_fraction',\n",
    "        y='start_f1',\n",
    "        size=100,\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    # Add titles and labels\n",
    "    g.set_axis_labels(\"Training Data Fraction\", \"F1 Score (Macro)\")\n",
    "    g.fig.suptitle(f\"Performance: Baseline vs. Augmented ({strategy} strategy)\", y=1.03, fontsize=18)\n",
    "    g.add_legend(title=\"Experiment Type\")\n",
    "    g.fig.tight_layout()\n",
    "    \n",
    "    # Save the figure with a unique name\n",
    "    g.fig.savefig(f\"my_results_plot_{strategy}.png\", bbox_inches='tight', dpi=150)\n",
    "    plt.close(g.fig) # Close the figure to free up memory before the next loop\n",
    "\n",
    "print(\"Plots have been saved to the directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
