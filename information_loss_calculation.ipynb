{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1808e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Make sure the sompy library files (sompy_isom.py, codebook.py, etc.)\n",
    "# are in the same directory as this notebook.\n",
    "from folder.sompy_isom import SOMFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4984487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " pca_linear_initialization took: 0.018000 seconds\n",
      " Factor loading: 0.985989\n",
      " som_lininit took: 0.040000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Information Loss Analysis for: airfoil_cl ---\n",
      "Analysis Complete for airfoil_cl. Total info preserved in 2D PCA: 53.85%\n",
      "\n",
      "--- Starting Information Loss Analysis for: airfoil_cl_m ---\n",
      "Analysis Complete for airfoil_cl_m. Total info preserved in 2D PCA: 53.85%\n",
      "\n",
      "--- Starting Information Loss Analysis for: framed_safety ---\n",
      "Analysis Complete for framed_safety. Total info preserved in 2D PCA: 17.88%\n",
      "\n",
      "--- Starting Information Loss Analysis for: framed_validity ---\n",
      "Analysis Complete for framed_validity. Total info preserved in 2D PCA: 16.83%\n",
      "\n",
      "--- Starting Information Loss Analysis for: solar_hex ---\n",
      "Analysis Complete for solar_hex. Total info preserved in 2D PCA: 100.00%\n",
      "\n",
      "--- Starting Information Loss Analysis for: welded_beam ---\n",
      "Analysis Complete for welded_beam. Total info preserved in 2D PCA: 51.02%\n",
      "\n",
      "--- Starting Information Loss Analysis for: welded_beam_balanced ---\n",
      "Analysis Complete for welded_beam_balanced. Total info preserved in 2D PCA: 55.86%\n",
      "\n",
      "\n",
      "===== SUMMARY OF INFORMATION PRESERVED IN 2D PCA =====\n",
      "                      total_info_in_2D_PCA\n",
      "dataset                                   \n",
      "solar_hex                       100.000000\n",
      "welded_beam_balanced             55.857816\n",
      "airfoil_cl                       53.851622\n",
      "airfoil_cl_m                     53.851622\n",
      "welded_beam                      51.022737\n",
      "framed_safety                    17.883889\n",
      "framed_validity                  16.827594\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ===================================================================\n",
    "# CELL 1: Helper Functions (from your previous notebook and this analysis)\n",
    "# ===================================================================\n",
    "\n",
    "def calculate_variable_importance(data):\n",
    "    \"\"\"Applies PCA to data and computes variable importance.\"\"\"\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    coeffs = pca.components_.T\n",
    "    weights = explained / np.sum(explained)\n",
    "    k = data.shape[1]\n",
    "    importance = np.sum(weights[:k] * np.abs(coeffs[:, :k]), axis=1)\n",
    "    importance /= np.sum(importance)\n",
    "    return importance, explained\n",
    "\n",
    "def analyze_isom_information_loss(X_df, y_s, mapsize=None, dataset_name=\"\"):\n",
    "    \"\"\"Performs the full information loss analysis and plots the results.\"\"\"\n",
    "    print(f\"\\n--- Starting Information Loss Analysis for: {dataset_name} ---\")\n",
    "    \n",
    "    # 1. PCA on Original Data\n",
    "    X_orig = X_df.values\n",
    "    d = X_orig.shape[1]\n",
    "    X_orig_norm = (X_orig - X_orig.mean(axis=0)) / X_orig.std(axis=0)\n",
    "    importance_orig, explained_orig = calculate_variable_importance(X_orig_norm)\n",
    "\n",
    "    # 2. Train iSOM\n",
    "    D = np.hstack([X_orig, y_s.values.reshape(-1, 1)])\n",
    "    sm = SOMFactory.build(D, mapsize=mapsize, normalization='range', initialization='pca')\n",
    "    sm.som_lininit()\n",
    "    sm.train(request_id=f'info_loss_{dataset_name}', verbose=None)\n",
    "    codebook = sm.codebook.matrix\n",
    "\n",
    "    # 3. PCA on iSOM Codebook\n",
    "    codebook_X = codebook[:, :-1]\n",
    "    importance_map, _ = calculate_variable_importance(codebook_X)\n",
    "\n",
    "    # 4. Compute Information Gain and Loss\n",
    "    total_info_gain_pca = np.sum(explained_orig[:2]) * 100\n",
    "    info_gain = total_info_gain_pca * importance_map\n",
    "    info_loss = (100 / d) - info_gain\n",
    "    perc_info_gain = 100 * info_gain / np.sum(info_gain)\n",
    "    perc_info_loss = 100 * info_loss / np.sum(info_loss)\n",
    "\n",
    "    # 5. Visualization\n",
    "    variable_indices = X_df.columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax1.bar(variable_indices, info_gain, label='Gain', color='tab:blue')\n",
    "    ax1.bar(variable_indices, info_loss, bottom=info_gain, label='Lost', color='tab:orange')\n",
    "    ax1.set_title(f'Info in Variables ({dataset_name})')\n",
    "    ax1.set_xlabel('Variable Index')\n",
    "    ax1.set_ylabel('Information')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.bar(variable_indices, perc_info_gain, label='% Gain', color='tab:blue')\n",
    "    ax2.bar(variable_indices, perc_info_loss, bottom=perc_info_gain, label='% Lost', color='tab:orange')\n",
    "    ax2.set_title(f'% Info in Variables ({dataset_name})')\n",
    "    ax2.set_xlabel('Variable Index')\n",
    "    ax2.set_ylabel('Percentage')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.suptitle(f'Information Loss Analysis for {dataset_name}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.savefig(f\"my_results_plot_{dataset_name}.png\", bbox_inches='tight', dpi=150)\n",
    "    plt.close(fig) # Close the figure to free up memory before the next loop\n",
    "    \n",
    "    print(f\"Analysis Complete for {dataset_name}. Total info preserved in 2D PCA: {total_info_gain_pca:.2f}%\")\n",
    "    return {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"total_info_in_2D_PCA\": total_info_gain_pca\n",
    "    }\n",
    "\n",
    "def read_data(path):\n",
    "    _, ext = os.path.splitext(path)\n",
    "    if ext == \".parquet\": return pd.read_parquet(path)\n",
    "    elif ext == \".csv\": return pd.read_csv(path, index_col=0)\n",
    "\n",
    "def load_data(base_path, dataset_name):\n",
    "    with open(os.path.join(base_path, dataset_name, f\"{dataset_name}.meta.json\")) as f: meta = json.load(f)\n",
    "    train_data = read_data(os.path.join(base_path, dataset_name, f\"{dataset_name}.{meta['format']}\"))\n",
    "    return train_data, meta\n",
    "\n",
    "# ===================================================================\n",
    "# CELL 2: Main Loop to Analyze All Datasets\n",
    "# ===================================================================\n",
    "\n",
    "BASE_DATA_PATH = \"data\"\n",
    "DATASETS_TO_RUN = [\n",
    "    \"airfoil_cl\", \"airfoil_cl_m\", \"framed_safety\", \"framed_validity\", \n",
    "    \"solar_hex\", \"welded_beam\", \"welded_beam_balanced\"\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for dataset_name in DATASETS_TO_RUN:\n",
    "    try:\n",
    "        # Load the data\n",
    "        full_train_data, meta = load_data(BASE_DATA_PATH, dataset_name)\n",
    "        y_column = meta[\"label\"]\n",
    "\n",
    "        # Binarize the target if it's continuous (as in your experiment notebook)\n",
    "        if pd.api.types.is_float_dtype(full_train_data[y_column]):\n",
    "            threshold = full_train_data[y_column].median()\n",
    "            full_train_data[y_column] = (full_train_data[y_column] <= threshold).astype(int)\n",
    "\n",
    "        # Separate features and target\n",
    "        X_data = full_train_data.drop(columns=[y_column])\n",
    "        y_data = full_train_data[y_column]\n",
    "        \n",
    "        # Use a reasonable subset for analysis to keep it fast\n",
    "        if len(full_train_data) > 1000:\n",
    "            data_subset = full_train_data.sample(n=1000, random_state=42)\n",
    "            X_data = data_subset.drop(columns=[y_column])\n",
    "            y_data = data_subset[y_column]\n",
    "\n",
    "        # Run the analysis for the current dataset\n",
    "        results = analyze_isom_information_loss(X_data, y_data, mapsize=[20, 20], dataset_name=dataset_name)\n",
    "        all_results.append(results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Could not process dataset {dataset_name}. Error: {e} !!!\")\n",
    "\n",
    "# --- 3. Print a summary of the total information preserved for each dataset ---\n",
    "print(\"\\n\\n===== SUMMARY OF INFORMATION PRESERVED IN 2D PCA =====\")\n",
    "summary_df = pd.DataFrame(all_results).set_index('dataset')\n",
    "print(summary_df.sort_values(by='total_info_in_2D_PCA', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25aea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
